{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1268f4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T09:21:05.863358Z",
     "iopub.status.busy": "2025-03-30T09:21:05.863128Z",
     "iopub.status.idle": "2025-03-30T09:22:03.086886Z",
     "shell.execute_reply": "2025-03-30T09:22:03.086151Z"
    },
    "papermill": {
     "duration": 57.231897,
     "end_time": "2025-03-30T09:22:03.088418",
     "exception": false,
     "start_time": "2025-03-30T09:21:05.856521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-30 09:21:58 __init__.py:183] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"TRITON_PTXAS_PATH\"] = \"/usr/local/cuda/bin/ptxas\"\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "from collections import Counter\n",
    "import numpy as np, pandas as pd, polars as pl\n",
    "\n",
    "import torch\n",
    "import vllm\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "import kaggle_evaluation.aimo_2_inference_server\n",
    "\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ebd43a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T09:22:03.098596Z",
     "iopub.status.busy": "2025-03-30T09:22:03.098359Z",
     "iopub.status.idle": "2025-03-30T09:22:03.110474Z",
     "shell.execute_reply": "2025-03-30T09:22:03.109885Z"
    },
    "papermill": {
     "duration": 0.018101,
     "end_time": "2025-03-30T09:22:03.111521",
     "exception": false,
     "start_time": "2025-03-30T09:22:03.093420",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(seed=5)\n",
    "\n",
    "start_time = time.time()\n",
    "cutoff_time = start_time + (4 * 60 + 50) * 60\n",
    "cutoff_times = [int(x) for x in np.linspace(cutoff_time, start_time + 60 * 60, 50 + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a52ca76",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-03-30T09:22:03.121397Z",
     "iopub.status.busy": "2025-03-30T09:22:03.121156Z",
     "iopub.status.idle": "2025-03-30T09:23:55.879251Z",
     "shell.execute_reply": "2025-03-30T09:23:55.878429Z"
    },
    "papermill": {
     "duration": 112.764858,
     "end_time": "2025-03-30T09:23:55.880814",
     "exception": false,
     "start_time": "2025-03-30T09:22:03.115956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-30 09:22:28 config.py:526] This model supports multiple tasks: {'embed', 'score', 'reward', 'classify', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 03-30 09:22:31 awq_marlin.py:109] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 03-30 09:22:32 config.py:1383] Defaulting to use mp for distributed inference\n",
      "WARNING 03-30 09:22:32 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "INFO 03-30 09:22:32 llm_engine.py:232] Initializing a V0 LLM engine (v0.7.1) with config: model='/kaggle/input/m/shelterw/deepseek-r1/transformers/light-r1-7b-ds-awq/1', speculative_config=None, tokenizer='/kaggle/input/m/shelterw/deepseek-r1/transformers/light-r1-7b-ds-awq/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=31000, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=2025, served_model_name=/kaggle/input/m/shelterw/deepseek-r1/transformers/light-r1-7b-ds-awq/1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":80}, use_cached_outputs=False, \n",
      "WARNING 03-30 09:22:32 multiproc_worker_utils.py:298] Reducing Torch parallelism from 24 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 03-30 09:22:32 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m INFO 03-30 09:22:32 multiproc_worker_utils.py:227] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m INFO 03-30 09:22:32 multiproc_worker_utils.py:227] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m INFO 03-30 09:22:32 multiproc_worker_utils.py:227] Worker ready; awaiting tasks\n",
      "WARNING 03-30 09:22:33 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m WARNING 03-30 09:22:33 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m WARNING 03-30 09:22:33 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m WARNING 03-30 09:22:33 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "INFO 03-30 09:22:33 cuda.py:235] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m INFO 03-30 09:22:33 cuda.py:235] Using Flash Attention backend.\n",
      "INFO 03-30 09:22:33 cuda.py:235] Using Flash Attention backend.\n",
      "INFO 03-30 09:22:33 cuda.py:235] Using Flash Attention backend.\n",
      "INFO 03-30 09:22:44 utils.py:938] Found nccl from library libnccl.so.2\n",
      "INFO 03-30 09:22:44 pynccl.py:67] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m INFO 03-30 09:22:44 utils.py:938] Found nccl from library libnccl.so.2\n",
      "INFO 03-30 09:22:44 utils.py:938] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m INFO 03-30 09:22:44 pynccl.py:67] vLLM is using nccl==2.21.5\n",
      "INFO 03-30 09:22:44 pynccl.py:67] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m INFO 03-30 09:22:44 utils.py:938] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m INFO 03-30 09:22:44 pynccl.py:67] vLLM is using nccl==2.21.5\n",
      "WARNING 03-30 09:22:45 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m WARNING 03-30 09:22:45 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 03-30 09:22:45 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 03-30 09:22:45 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 03-30 09:22:45 shm_broadcast.py:256] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_812001df'), local_subscribe_port=41301, remote_subscribe_port=None)\n",
      "INFO 03-30 09:22:45 model_runner.py:1111] Starting to load model /kaggle/input/m/shelterw/deepseek-r1/transformers/light-r1-7b-ds-awq/1...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m INFO 03-30 09:22:45 model_runner.py:1111] Starting to load model /kaggle/input/m/shelterw/deepseek-r1/transformers/light-r1-7b-ds-awq/1...\n",
      "INFO 03-30 09:22:45 model_runner.py:1111] Starting to load model /kaggle/input/m/shelterw/deepseek-r1/transformers/light-r1-7b-ds-awq/1...\n",
      "INFO 03-30 09:22:45 model_runner.py:1111] Starting to load model /kaggle/input/m/shelterw/deepseek-r1/transformers/light-r1-7b-ds-awq/1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10cc4410fbdd459ebc661fb7d9296be5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-30 09:23:13 model_runner.py:1116] Loading model weights took 1.3375 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m INFO 03-30 09:23:13 model_runner.py:1116] Loading model weights took 1.3375 GB\n",
      "INFO 03-30 09:23:13 model_runner.py:1116] Loading model weights took 1.3375 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m INFO 03-30 09:23:13 model_runner.py:1116] Loading model weights took 1.3375 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m WARNING 03-30 09:23:35 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "WARNING 03-30 09:23:35 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m WARNING 03-30 09:23:35 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "WARNING 03-30 09:23:35 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "WARNING 03-30 09:23:35 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m INFO 03-30 09:23:35 worker.py:266] Memory profiling takes 21.23 seconds\r\n",
      "INFO 03-30 09:23:35 worker.py:266] Memory profiling takes 21.23 seconds\r\n",
      "WARNING 03-30 09:23:35 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m INFO 03-30 09:23:35 worker.py:266] the current vLLM instance can use total_gpu_memory (22.28GiB) x gpu_memory_utilization (0.95) = 21.16GiB\r\n",
      "INFO 03-30 09:23:35 worker.py:266] the current vLLM instance can use total_gpu_memory (22.28GiB) x gpu_memory_utilization (0.95) = 21.16GiB\r\n",
      "INFO 03-30 09:23:35 worker.py:266] Memory profiling takes 21.23 seconds\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m INFO 03-30 09:23:35 worker.py:266] model weights take 1.34GiB; non_torch_memory takes 0.17GiB; PyTorch activation peak memory takes 1.94GiB; the rest of the memory reserved for KV Cache is 17.72GiB.\n",
      "INFO 03-30 09:23:35 worker.py:266] model weights take 1.34GiB; non_torch_memory takes 0.17GiB; PyTorch activation peak memory takes 1.94GiB; the rest of the memory reserved for KV Cache is 17.72GiB.\n",
      "INFO 03-30 09:23:35 worker.py:266] the current vLLM instance can use total_gpu_memory (22.28GiB) x gpu_memory_utilization (0.95) = 21.16GiB\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m INFO 03-30 09:23:35 worker.py:266] model weights take 1.34GiB; non_torch_memory takes 0.17GiB; PyTorch activation peak memory takes 1.94GiB; the rest of the memory reserved for KV Cache is 17.72GiB.\n",
      "WARNING 03-30 09:23:35 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "WARNING 03-30 09:23:35 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "INFO 03-30 09:23:35 worker.py:266] Memory profiling takes 21.57 seconds\r\n",
      "INFO 03-30 09:23:35 worker.py:266] the current vLLM instance can use total_gpu_memory (22.28GiB) x gpu_memory_utilization (0.95) = 21.16GiB\r\n",
      "INFO 03-30 09:23:35 worker.py:266] model weights take 1.34GiB; non_torch_memory takes 0.17GiB; PyTorch activation peak memory takes 1.94GiB; the rest of the memory reserved for KV Cache is 17.72GiB.\n",
      "INFO 03-30 09:23:36 executor_base.py:108] # CUDA blocks: 82961, # CPU blocks: 18724\n",
      "INFO 03-30 09:23:36 executor_base.py:113] Maximum concurrency for 31000 tokens per request: 42.82x\n",
      "WARNING 03-30 09:23:36 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "WARNING 03-30 09:23:36 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m WARNING 03-30 09:23:36 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "WARNING 03-30 09:23:36 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "WARNING 03-30 09:23:36 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m WARNING 03-30 09:23:36 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "WARNING 03-30 09:23:36 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "WARNING 03-30 09:23:36 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "INFO 03-30 09:23:41 model_runner.py:1435] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m INFO 03-30 09:23:41 model_runner.py:1435] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 03-30 09:23:41 model_runner.py:1435] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:   0%|          | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-30 09:23:41 model_runner.py:1435] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  92%|█████████▏| 12/13 [00:11<00:00,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=354)\u001b[0;0m "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 13/13 [00:12<00:00,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-30 09:23:53 model_runner.py:1563] Graph capturing finished in 13 secs, took 0.16 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m INFO 03-30 09:23:53 model_runner.py:1563] Graph capturing finished in 13 secs, took 0.16 GiB\n",
      "INFO 03-30 09:23:53 model_runner.py:1563] Graph capturing finished in 13 secs, took 0.16 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m INFO 03-30 09:23:53 model_runner.py:1563] Graph capturing finished in 13 secs, took 0.16 GiB\n",
      "INFO 03-30 09:23:53 llm_engine.py:429] init engine (profile, create kv cache, warmup model) took 39.85 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if os.getenv('KAGGLE_KERNEL_RUN_TYPE') or os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    llm_model_pth = '/kaggle/input/m/shelterw/deepseek-r1/transformers/light-r1-7b-ds-awq/1'\n",
    "else:\n",
    "    llm_model_pth = '/kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-7b-awq-casperhansen/1'\n",
    "\n",
    "MAX_NUM_SEQS = 75\n",
    "MAX_MODEL_LEN = 31000\n",
    "\n",
    "llm = LLM(\n",
    "    llm_model_pth,\n",
    "    #dtype=\"half\",                 # The data type for the model weights and activations\n",
    "    max_num_seqs=MAX_NUM_SEQS,    # Maximum number of sequences per iteration. Default is 256\n",
    "    max_model_len=MAX_MODEL_LEN,  # Model context length\n",
    "    trust_remote_code=True,       # Trust remote code (e.g., from HuggingFace) when downloading the model and tokenizer\n",
    "    tensor_parallel_size=4,       # The number of GPUs to use for distributed execution with tensor parallelism\n",
    "    gpu_memory_utilization=0.95,  # The ratio (between 0 and 1) of GPU memory to reserve for the model\n",
    "    seed=2025,\n",
    ")\n",
    "\n",
    "tokenizer = llm.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c83f4040",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T09:23:55.905621Z",
     "iopub.status.busy": "2025-03-30T09:23:55.905369Z",
     "iopub.status.idle": "2025-03-30T09:23:55.910751Z",
     "shell.execute_reply": "2025-03-30T09:23:55.910100Z"
    },
    "papermill": {
     "duration": 0.018853,
     "end_time": "2025-03-30T09:23:55.911826",
     "exception": false,
     "start_time": "2025-03-30T09:23:55.892973",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#base_system_prompt = \"\"\"You are a helpful and harmless math assistant. You should learn transferable problem-solving strategies from the given \"Example\", rather than memorizing specific instances. Break this question into parts. Take quality time to Verify every case and condition in the intermediate step. Iterate over every value of n from the least to the highest value without any assumptions or generalizations. CRUCIAL: Your response MUST END with exactly one \\\\boxed{} containing the final answer after modulo 1000. Don't stop generating until you reach this \\\\boxed{} answer.\\n\\n\"\"\"\n",
    "\n",
    "TOPIC_PROMPTS = {\n",
    "    \"Number Theory\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Break this question down first. Carefully Analyze and follow the problem statement exactly . Avoid: (- mis-calculations, - overly rough approximations, -imprecise handling of the radical–axis relations, - misapplication of an alternate counting method that over‐counted by introducing unnecessary factors, - incorrect generalization of the sum‐of‐digits formula over the whole range). **IMPORTANT**: Arrive at early answers. Before using an alternative method, Put possible answer within \\\\boxed{} after taking modulo 1000.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"You MUST IGNORE  theoretical reasoning. You MUST TAKE QUALITY TIME TO PERFORM A real-time exhaustive verification of ALL CASES using Sagemath or Sympy. **IMPORTANT**: Arrive at early answers. Before using an alternative method, Put ALL possible answers within \\\\boxed{} after taking modulo 1000.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Don't solve, ONLY take time to verify all search cases to the 7th case then STOP QUICKLY. Summarize with verified steps. NOTE: There are other cases that meet the condition after the 7th case, but because of time constraints and computational limitations, stop at the 7th case and find a pattern that satisfies other cases. You MUST AVOID initial rough search that might hint at a potential candidate, a more rigorous and detailed analysis is needed. DON'T hypothesize. **IMPORTANT**: Arrive at early answers. Before using an alternative method, Put ALL possible answers within \\\\boxed{} after taking modulo 1000.\"\n",
    "        }\n",
    "        \n",
    "    ],\n",
    "    \n",
    "    \"Geometry\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Break this question down first. Carefully Analyze and follow the problem statement exactly . Avoid: (- mis-calculations, - overly rough approximations, -imprecise handling of the radical–axis relations, - misapplication of an alternate counting method that over‐counted by introducing unnecessary factors, - incorrect generalization of the sum‐of‐digits formula over the whole range). **IMPORTANT**: Arrive at early answers. Before using an alternative method, Put possible answer within \\\\boxed{} after taking modulo 1000.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Break this question down first. Analyze Verify  Conditions. Solve. No mis-calculations. No overly rough approximations. No imprecise handling of the radical–axis relations. Take modulo 1000 of final answer.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Break this question down first, then analyze and verify every condition and solve it. Construct geometric diagrams and apply geometric principles, Don't make any assumptions. Avoid miscalculations when applying Theorems, and overly rough approximations and an imprecise handling of the radical–axis relations. **IMPORTANT**: Arrive at early answers. Before using an alternative method, Put possible answer within \\\\boxed{} after taking modulo 1000.\"\n",
    "        }\n",
    "    ],\n",
    "    \n",
    "    \"Combinatorics\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Break this question down first. Carefully Analyze and follow the problem statement exactly . Avoid: (- mis-calculations, - overly rough approximations, -imprecise handling of the radical–axis relations, - misapplication of an alternate counting method that over‐counted by introducing unnecessary factors, - incorrect generalization of the sum‐of‐digits formula over the whole range). **IMPORTANT**: Arrive at early answers. Before using an alternative method, Put possible answer within \\\\boxed{} after taking modulo 1000.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"You must Take quality time to Verify every small case for optimal accuracy with Exhaustive CAS Checks without making any assumptions in the question you will receive. If you don't verify every small case accurately, don't solve anything else. There should be NO OVERSIGHT OR ASSUMPTION DUE TO GENERALIZATION. Summarize what you have done so far making sure every calculation is VERY ACCURATE!!! Complete the solution and arrive at the ONLY accurate answer after taking modulo 1000. Put your final answer within \\\\boxed{}.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Solve this question step by step. Recheck every critical step for optimal accuracy. Do not mis‐simplify any expression. Summarize what you have done so far. Complete the solution. Take modulo 1000 of final answer. **IMPORTANT**: Put your final answer within \\\\boxed{}.\"\n",
    "        }\n",
    "    ],\n",
    "    \n",
    "    \"Algebra/Modular Arithmetic\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Break this question down first. Carefully Analyze and follow the problem statement exactly . Avoid: (- mis-calculations, - overly rough approximations, -imprecise handling of the radical–axis relations, - misapplication of an alternate counting method that over‐counted by introducing unnecessary factors, - incorrect generalization of the sum‐of‐digits formula over the whole range). **IMPORTANT**: Arrive at early answers. Before using an alternative method, Put possible answer within \\\\boxed{} after taking modulo 1000.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Solve this question step by step. Recheck every critical step for optimal accuracy. Do not mis‐simplify any expression. Summarize what you have done so far. Complete the solution. Take modulo 1000 of final answer. **IMPORTANT**: Put your final answer within \\\\boxed{}.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"You must Take quality time to Verify every small case for optimal accuracy with Exhaustive CAS Checks without making any assumptions in the question you will receive. If you don't verify every small case accurately, don't solve anything else. There should be NO OVERSIGHT OR ASSUMPTION DUE TO GENERALIZATION. Summarize what you have done so far making sure every calculation is VERY ACCURATE!!! Complete the solution and arrive at the ONLY accurate answer after taking modulo 1000. Put your final answer within \\\\boxed{}.\"\n",
    "        }\n",
    "        \n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8963e0c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T09:23:55.935443Z",
     "iopub.status.busy": "2025-03-30T09:23:55.935216Z",
     "iopub.status.idle": "2025-03-30T09:23:55.942910Z",
     "shell.execute_reply": "2025-03-30T09:23:55.942330Z"
    },
    "papermill": {
     "duration": 0.020694,
     "end_time": "2025-03-30T09:23:55.943965",
     "exception": false,
     "start_time": "2025-03-30T09:23:55.923271",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def detect_topic(question):\n",
    "    question = question.lower()\n",
    "    \n",
    "    topic_patterns = {\n",
    "        \"Number Theory\": {\n",
    "            \"keywords\": {\"gcd\", \"lcm\", \"mod\", \"divisible\", \"prime\", \"congruence\", \n",
    "                         \"remainder\", \"factor\", \"multiple\", \"coprime\", \"composite\",\n",
    "                         \"diophantine\", \"euclidean\", \"bezout\", \"chinese remainder\",\n",
    "                         \"greatest common divisors\", \"sum of digits\", \"digit sum\", \n",
    "                         \"s(n)\", \"base 10\", \"digits\"},\n",
    "            \"weight\": 1.2,\n",
    "            \"regex\": r\"mod(ulo)?\\s*\\d+|prime\\s*power|≡\\s*\\d+\\s*\\(mod|sum\\s+of\\s+digits\"\n",
    "        },\n",
    "        \"Combinatorics\": {\n",
    "            \"keywords\": {\"arrange\", \"combination\", \"permutation\", \"probability\", \n",
    "                        \"graph\", \"tree\", \"path\", \"subset\", \"binomial\", \"pigeonhole\",\n",
    "                        \"inclusion-exclusion\", \"recurrence\", \"generating function\"},\n",
    "            \"weight\": 1.1,\n",
    "            \"regex\": r\"\\d+\\s*ways|arrang(e|ing).* (not|never)\"\n",
    "        },\n",
    "        \"Geometry\": {\n",
    "            \"keywords\": {\"triangle\", \"circle\", \"coordinate\", \"volume\", \"area\", \n",
    "                        \"circumradius\", \"altitude\", \"hypotenuse\", \"parabola\", \n",
    "                        \"ellipse\", \"theorem\", \"bisector\", \"bisects\", \n",
    "                        \"circumcircle of triangle\", \"orthocenter\"},\n",
    "            \"weight\": 1.0,\n",
    "            \"regex\": r\"\\b(r|d)\\s*=\\s*\\d+|foot\\s+of\\s+perpendicular\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    special_cases = {\n",
    "        \"Number Theory\": [\n",
    "            r\"\\bs\\(n\\)\\b\",\n",
    "            r\"sum\\s+of\\s+digits\",\n",
    "            r\"base\\s+10\",\n",
    "            r\"\\d+day\",\n",
    "            r\"polynomial\\s+congruence\",\n",
    "            r\"exponential\\s+mod\",\n",
    "            r\"remainder\\s+when\\s+divided\"\n",
    "        ],\n",
    "        \"Geometry\": [\n",
    "            r\"circumradius\\s*=\\s*\\d+\",\n",
    "            r\"triangle.*side\\s+lengths\",\n",
    "            r\"volume\\s+of\\s+cylinder\"\n",
    "        ],\n",
    "        \"Combinatorics\": [\n",
    "            r\"at\\s+least\\s+\\d+\\s*heads\",\n",
    "            r\"probability\\s+of\\s+winning\",\n",
    "            r\"vertices\\s+and\\s+edges\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    scores = {topic: 0 for topic in topic_patterns}\n",
    "    \n",
    "    # Base scoring\n",
    "    for topic, data in topic_patterns.items():\n",
    "        # Keyword matches\n",
    "        kw_matches = len([kw for kw in data[\"keywords\"] if kw in question])\n",
    "        scores[topic] += kw_matches * data[\"weight\"]\n",
    "        \n",
    "        # Regex matches\n",
    "        if \"regex\" in data:\n",
    "            scores[topic] += len(re.findall(data[\"regex\"], question)) * 2\n",
    "    \n",
    "    # Special case boosts\n",
    "    for topic, patterns in special_cases.items():\n",
    "        for pattern in patterns:\n",
    "            if re.search(pattern, question, re.IGNORECASE):\n",
    "                scores[topic] += 3\n",
    "    \n",
    "    # Priority tiebreaker\n",
    "    priority_order = [\n",
    "        \"Number Theory\",\n",
    "        \"Combinatorics\", \n",
    "        \"Geometry\"\n",
    "    ]\n",
    "    \n",
    "    max_score = max(scores.values())\n",
    "    candidates = [t for t, s in scores.items() if s == max_score]\n",
    "    \n",
    "    for topic in priority_order:\n",
    "        if topic in candidates:\n",
    "            return topic\n",
    "    \n",
    "    return \"Number Theory\"  # Fallback to highest priority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "439aa9c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T09:23:55.967306Z",
     "iopub.status.busy": "2025-03-30T09:23:55.967078Z",
     "iopub.status.idle": "2025-03-30T09:23:55.972622Z",
     "shell.execute_reply": "2025-03-30T09:23:55.972020Z"
    },
    "papermill": {
     "duration": 0.018327,
     "end_time": "2025-03-30T09:23:55.973657",
     "exception": false,
     "start_time": "2025-03-30T09:23:55.955330",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_boxed_text(text):\n",
    "    pattern = r'\\\\boxed{([^}]*)}'\n",
    "    matches = re.findall(pattern, text)\n",
    "    if not matches:\n",
    "        return \"\"\n",
    "    for match in matches[::-1]:\n",
    "        if match != \"\":\n",
    "            return match\n",
    "    return \"\"\n",
    "\n",
    "def batch_message_filter(list_of_messages) -> tuple[list[list[dict]], list[str]]:\n",
    "    extracted_answers = []\n",
    "    list_of_messages_to_keep = []\n",
    "    for messages in list_of_messages:\n",
    "        answer = extract_boxed_text(messages[-1]['content'])\n",
    "        if answer:\n",
    "            extracted_answers.append(answer)\n",
    "        else:\n",
    "            list_of_messages_to_keep.append(messages)\n",
    "    return list_of_messages_to_keep, extracted_answers\n",
    "\n",
    "def select_answer(answers):\n",
    "    counter = Counter()\n",
    "    for answer in answers:\n",
    "        try:\n",
    "            if int(answer) == float(answer):\n",
    "                counter[int(answer)] += 1 + random.random() / 1_000\n",
    "        except:\n",
    "            pass\n",
    "    if not counter:\n",
    "        return 210\n",
    "    _, answer = sorted([(v,k) for k,v in counter.items()], reverse=True)[0]\n",
    "    return answer%1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5ecf348",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T09:23:55.996655Z",
     "iopub.status.busy": "2025-03-30T09:23:55.996441Z",
     "iopub.status.idle": "2025-03-30T09:23:56.001616Z",
     "shell.execute_reply": "2025-03-30T09:23:56.001020Z"
    },
    "papermill": {
     "duration": 0.017826,
     "end_time": "2025-03-30T09:23:56.002660",
     "exception": false,
     "start_time": "2025-03-30T09:23:55.984834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch_message_generate(list_of_messages) -> list[list[dict]]:\n",
    "    # Enforce single-sequence processing for iterative refinement\n",
    "    assert len(list_of_messages) == 1, \"Use sequential mode for iterative prompts\"\n",
    "    \n",
    "    # Dynamic token limits based on remaining time\n",
    "    max_tokens = MAX_MODEL_LEN\n",
    "    \n",
    "\n",
    "    # Configure sampling parameters\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.6,\n",
    "        top_p=0.95,\n",
    "        min_p=0.05,\n",
    "        skip_special_tokens=True,\n",
    "        max_tokens=max_tokens,\n",
    "        stop=[\"</think>\"],\n",
    "        seed=777,\n",
    "    )\n",
    "\n",
    "    # Convert message chain to prompt text\n",
    "    list_of_texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            conversation=messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        for messages in list_of_messages\n",
    "    ]\n",
    "\n",
    "    # Generate response using LLM\n",
    "    request_output = llm.generate(\n",
    "        prompts=list_of_texts,\n",
    "        sampling_params=sampling_params,\n",
    "    )\n",
    "\n",
    "    # Update message chain with generated response\n",
    "    updated_messages = []\n",
    "    for messages, single_request_output in zip(list_of_messages, request_output):\n",
    "        # Append assistant's response to message history\n",
    "        messages.append({\n",
    "            'role': 'assistant', \n",
    "            'content': single_request_output.outputs[0].text\n",
    "        })\n",
    "        \n",
    "        # Store token count for sorting (maintains compatibility)\n",
    "        updated_messages.append((\n",
    "            len(single_request_output.outputs[0].token_ids),\n",
    "            messages\n",
    "        ))\n",
    "\n",
    "    # Sort by generated response length (ascending)\n",
    "    updated_messages.sort(key=lambda x: x[0])\n",
    "    \n",
    "    # Return only the message chain (drop length metadata)\n",
    "    return [messages for _, messages in updated_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "430203e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T09:23:56.025923Z",
     "iopub.status.busy": "2025-03-30T09:23:56.025711Z",
     "iopub.status.idle": "2025-03-30T09:23:56.028331Z",
     "shell.execute_reply": "2025-03-30T09:23:56.027725Z"
    },
    "papermill": {
     "duration": 0.015442,
     "end_time": "2025-03-30T09:23:56.029398",
     "exception": false,
     "start_time": "2025-03-30T09:23:56.013956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#prompts = [#\"You must Take quality time to Verify every small cases upto the 7th value before assuming a pattern. There should be NO OVERSIGHT OR ASSUMPTION DUE TO GENERALIZATION. Summarize what you have done so far making sure every calculation is VERY ACCURATE!!! Complete the solution and arrive at the ONLY accurate answer after taking modulo 1000. Put your final answer within \\\\boxed{}.\",\n",
    "           #\"Solve this question step by step. Recheck every critical step for optimal accuracy. Do not mis‐simplify any expression. Summarize what you have done so far. Complete the solution. Take modulo 1000 of final answer. **IMPORTANT**: Put your final answer within \\\\boxed{}.\",\n",
    "           #\"Break this question down first. Carefully Analyze and follow the problem statement exactly . Avoid: (- mis-calculations, - overly rough approximations, -imprecise handling of the radical–axis relations, - misapplication of an alternate counting method that over‐counted by introducing unnecessary factors, - incorrect generalization of the sum‐of‐digits formula over the whole range). **IMPORTANT**: Arrive at early answers. Before using an alternative method, Put possible answer within \\\\boxed{} after taking modulo 1000.\",\n",
    "           #\"You MUST IGNORE  theoretical reasoning. You MUST TAKE QUALITY TIME TO PERFORM A real-time exhaustive verification of ALL CASEs using Sagemath or Sympy. **IMPORTANT**: Arrive at early answers. Before using an alternative method, Put possible answer within \\\\boxed{} after taking modulo 1000.\"\n",
    "          #]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59d782c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T09:23:56.052937Z",
     "iopub.status.busy": "2025-03-30T09:23:56.052725Z",
     "iopub.status.idle": "2025-03-30T09:23:56.057773Z",
     "shell.execute_reply": "2025-03-30T09:23:56.057192Z"
    },
    "papermill": {
     "duration": 0.018061,
     "end_time": "2025-03-30T09:23:56.058806",
     "exception": false,
     "start_time": "2025-03-30T09:23:56.040745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_for_question(question: str) -> int:\n",
    "    if time.time() > cutoff_time:\n",
    "        return 210\n",
    "    \n",
    "\n",
    "    question += \" When solving a problem, YOU MUST follow the problem statement exactly and avoid switching to alternate methods that introduce extra factors or over-counting.. YOU MUST put ALL the suspected answers in \\\\boxed{} before verifying further\"\n",
    "    \n",
    "\n",
    "    topic = detect_topic(question)\n",
    "    prompts = TOPIC_PROMPTS.get(topic, TOPIC_PROMPTS[\"Number Theory\"])\n",
    "    all_extracted_answers = []\n",
    "\n",
    "    # Create 3 INDEPENDENT message threads\n",
    "    for i in range(3):\n",
    "        # Fresh message history for each prompt\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"\"},\n",
    "            {\"role\": \"user\", \"content\": f\"{prompts[i]}\\n\\n{question}\"}\n",
    "        ]\n",
    "        \n",
    "        # Generate and print response\n",
    "        messages = batch_message_generate([messages])[0]\n",
    "        \n",
    "        print(f\"\\n===== PROMPT {i+1} RESPONSE =====\")\n",
    "        print(f\"Prompt Config: {prompts[i][:100]}...\")  # Show first 100 chars of prompt\n",
    "        print(f\"Generated Response: {messages[-1]['content'][-500:]}...\")# Show last 500 chars of response\n",
    "        print(\"-\"*70)\n",
    "\n",
    "        # Extract and store answer\n",
    "        _, extracted = batch_message_filter([messages])\n",
    "        all_extracted_answers.extend(extracted)\n",
    "        print(f\"Extracted answers: {all_extracted_answers}\")\n",
    "    \n",
    "    \n",
    "        \n",
    "        # Reset context while keeping system prompt\n",
    "        messages = [messages[0]] + [messages[-1]]  # Carry final response forward if needed\n",
    "\n",
    "    answer = select_answer(all_extracted_answers)\n",
    "    print(f\"Answer: {answer}\")\n",
    "    return answer % 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a060aa4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T09:23:56.082189Z",
     "iopub.status.busy": "2025-03-30T09:23:56.081954Z",
     "iopub.status.idle": "2025-03-30T09:23:56.085419Z",
     "shell.execute_reply": "2025-03-30T09:23:56.084830Z"
    },
    "papermill": {
     "duration": 0.016348,
     "end_time": "2025-03-30T09:23:56.086484",
     "exception": false,
     "start_time": "2025-03-30T09:23:56.070136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(id_: pl.DataFrame, question: pl.DataFrame) -> pl.DataFrame | pd.DataFrame:\n",
    "    id_ = id_.item(0)\n",
    "    print(\"------\")\n",
    "    print(id_)\n",
    "    question = question.item(0)\n",
    "    answer = predict_for_question(question)\n",
    "    print(question)\n",
    "    print(\"------\\n\\n\")\n",
    "    return pl.DataFrame({'id': id_, 'answer': answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e4360bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T09:23:56.109788Z",
     "iopub.status.busy": "2025-03-30T09:23:56.109582Z",
     "iopub.status.idle": "2025-03-30T09:23:56.179249Z",
     "shell.execute_reply": "2025-03-30T09:23:56.178565Z"
    },
    "papermill": {
     "duration": 0.082676,
     "end_time": "2025-03-30T09:23:56.180494",
     "exception": false,
     "start_time": "2025-03-30T09:23:56.097818",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.read_csv(\n",
    "    '/kaggle/input/ai-mathematical-olympiad-progress-prize-2/reference.csv'\n",
    ").drop('answer', axis=1).to_csv('reference.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e51d8063",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T09:23:56.203745Z",
     "iopub.status.busy": "2025-03-30T09:23:56.203534Z",
     "iopub.status.idle": "2025-03-30T09:25:08.695792Z",
     "shell.execute_reply": "2025-03-30T09:25:08.694773Z"
    },
    "papermill": {
     "duration": 72.505051,
     "end_time": "2025-03-30T09:25:08.696886",
     "exception": true,
     "start_time": "2025-03-30T09:23:56.191835",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "a1d40b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [01:11<00:00, 71.82s/it, est. speed input: 4.05 toks/s, output: 103.81 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== PROMPT 1 RESPONSE =====\n"
     ]
    },
    {
     "ename": "GatewayRuntimeError",
     "evalue": "(<GatewayRuntimeErrorType.SERVER_RAISED_EXCEPTION: 3>, \"unhashable type: 'slice'\")",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mGatewayRuntimeError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-a045a0ac7c05>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0minference_server\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     inference_server.run_local_gateway(\n\u001b[0m\u001b[1;32m      6\u001b[0m         (\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#            '/kaggle/input/ai-mathematical-olympiad-progress-prize-2/test.csv',\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kaggle/input/ai-mathematical-olympiad-progress-prize-2/kaggle_evaluation/core/templates.py\u001b[0m in \u001b[0;36mrun_local_gateway\u001b[0;34m(self, data_paths, file_share_dir, *args, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kaggle/input/ai-mathematical-olympiad-progress-prize-2/kaggle_evaluation/core/templates.py\u001b[0m in \u001b[0;36mrun_local_gateway\u001b[0;34m(self, data_paths, file_share_dir, *args, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_gateway_for_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_share_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kaggle/input/ai-mathematical-olympiad-progress-prize-2/kaggle_evaluation/core/templates.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;31m# For local testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kaggle/input/ai-mathematical-olympiad-progress-prize-2/kaggle_evaluation/core/templates.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_data_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_submission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mkaggle_evaluation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_gateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGatewayRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgre\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kaggle/input/ai-mathematical-olympiad-progress-prize-2/kaggle_evaluation/core/templates.py\u001b[0m in \u001b[0;36mget_all_predictions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mall_row_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdata_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_ids\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_data_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdata_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_prediction_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mall_predictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kaggle/input/ai-mathematical-olympiad-progress-prize-2/kaggle_evaluation/core/templates.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predict'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_server_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'predict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_response_timeout_seconds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_seconds\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kaggle/input/ai-mathematical-olympiad-progress-prize-2/kaggle_evaluation/core/base_gateway.py\u001b[0m in \u001b[0;36mhandle_server_error\u001b[0;34m(self, exception, endpoint)\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0mmessage_match\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\"Exception calling application: (.*)\"'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexception_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIGNORECASE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage_match\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmessage_match\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mexception_str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mGatewayRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGatewayRuntimeErrorType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSERVER_RAISED_EXCEPTION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_channel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_InactiveRpcError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mGatewayRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGatewayRuntimeErrorType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSERVER_CONNECTION_FAILED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexception_str\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mGatewayRuntimeError\u001b[0m: (<GatewayRuntimeErrorType.SERVER_RAISED_EXCEPTION: 3>, \"unhashable type: 'slice'\")"
     ]
    }
   ],
   "source": [
    "inference_server = kaggle_evaluation.aimo_2_inference_server.AIMO2InferenceServer(predict)\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(\n",
    "        (\n",
    "#            '/kaggle/input/ai-mathematical-olympiad-progress-prize-2/test.csv',\n",
    "            'reference.csv',\n",
    "        )\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaL4",
   "dataSources": [
    {
     "databundleVersionId": 11376393,
     "sourceId": 86023,
     "sourceType": "competition"
    },
    {
     "sourceId": 220483900,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 39561,
     "modelInstanceId": 27950,
     "sourceId": 33384,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 169361,
     "modelInstanceId": 146846,
     "sourceId": 172510,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 224053,
     "modelInstanceId": 206829,
     "sourceId": 242129,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 224071,
     "modelInstanceId": 243776,
     "sourceId": 284463,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 251.968465,
   "end_time": "2025-03-30T09:25:14.204300",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-30T09:21:02.235835",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "041c552ed2fe468abe3e68a5cf3e0d02": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "10cc4410fbdd459ebc661fb7d9296be5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_b3c0c5aba8494126b4a6a996553b3c84",
        "IPY_MODEL_e67b0b18dcf0481aa3e4ea2bded959cc",
        "IPY_MODEL_3201e31ed35e413dada4e667acbb3895"
       ],
       "layout": "IPY_MODEL_e63f4d3be97e475f915ec43d73496374",
       "tabbable": null,
       "tooltip": null
      }
     },
     "3201e31ed35e413dada4e667acbb3895": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6c36cd8bb96043a68fe4cedec706967d",
       "placeholder": "​",
       "style": "IPY_MODEL_041c552ed2fe468abe3e68a5cf3e0d02",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:27&lt;00:00, 12.19s/it]\n"
      }
     },
     "5a3d04d33a8f49a49f6e4bec8395e25e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6c36cd8bb96043a68fe4cedec706967d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b3c0c5aba8494126b4a6a996553b3c84": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_fdc4910932db4a4f856dcb0ea09ac9c0",
       "placeholder": "​",
       "style": "IPY_MODEL_5a3d04d33a8f49a49f6e4bec8395e25e",
       "tabbable": null,
       "tooltip": null,
       "value": ""
      }
     },
     "d9e15ecea491442f9856fdd7e769f3a4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e471d1a79b8848deba720c2220f8810b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "e63f4d3be97e475f915ec43d73496374": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e67b0b18dcf0481aa3e4ea2bded959cc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d9e15ecea491442f9856fdd7e769f3a4",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e471d1a79b8848deba720c2220f8810b",
       "tabbable": null,
       "tooltip": null,
       "value": 2.0
      }
     },
     "fdc4910932db4a4f856dcb0ea09ac9c0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
