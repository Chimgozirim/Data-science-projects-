{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":86023,"databundleVersionId":11376393,"sourceType":"competition"},{"sourceId":220483900,"sourceType":"kernelVersion"},{"sourceId":33384,"sourceType":"modelInstanceVersion","modelInstanceId":27950,"modelId":39561},{"sourceId":172510,"sourceType":"modelInstanceVersion","modelInstanceId":146846,"modelId":169361},{"sourceId":242129,"sourceType":"modelInstanceVersion","modelInstanceId":206829,"modelId":224053},{"sourceId":284463,"sourceType":"modelInstanceVersion","modelInstanceId":243776,"modelId":224071}],"dockerImageVersionId":30840,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":1915.293623,"end_time":"2024-12-14T00:30:49.092769","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-12-13T23:58:53.799146","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"255af715ce8746cdada24bd57410a109":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"394ecaf7716f4d258e338f875ae4c835":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f09de95b95f64817b8bdf28c7278a20d","placeholder":"​","style":"IPY_MODEL_bbd7a2e18f6d4a3a9642a3827b9ced38","value":""}},"41c7e147e597489b8791785b0472f4f0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5a26c2110bbc4546b8d2d74aa636bdf9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a3cde1f4e394b269d342d937829dd4f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a26c2110bbc4546b8d2d74aa636bdf9","max":5,"min":0,"orientation":"horizontal","style":"IPY_MODEL_255af715ce8746cdada24bd57410a109","value":5}},"8ba53add104149c4ba88b4138fd4498e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e268c538e6a48979b263677cbdaf279":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8ba53add104149c4ba88b4138fd4498e","placeholder":"​","style":"IPY_MODEL_41c7e147e597489b8791785b0472f4f0","value":"Loading safetensors checkpoint shards: 100% Completed | 5/5 [02:13&lt;00:00, 28.89s/it]\n"}},"9e605e9455c24c8cac0ea9521feb3016":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_394ecaf7716f4d258e338f875ae4c835","IPY_MODEL_6a3cde1f4e394b269d342d937829dd4f","IPY_MODEL_8e268c538e6a48979b263677cbdaf279"],"layout":"IPY_MODEL_c1e5f9f23809451d918b10f2a03658c2"}},"bbd7a2e18f6d4a3a9642a3827b9ced38":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c1e5f9f23809451d918b10f2a03658c2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f09de95b95f64817b8bdf28c7278a20d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"TRITON_PTXAS_PATH\"] = \"/usr/local/cuda/bin/ptxas\"\nimport re\nimport time\nimport random\nimport warnings\nfrom collections import Counter\nimport numpy as np, pandas as pd, polars as pl\n\nimport torch\nimport vllm\nfrom vllm import LLM, SamplingParams\n\nimport kaggle_evaluation.aimo_2_inference_server\n\nwarnings.simplefilter('ignore')","metadata":{"papermill":{"duration":24.379545,"end_time":"2024-12-13T23:59:21.97668","exception":false,"start_time":"2024-12-13T23:58:57.597135","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T16:33:26.041805Z","iopub.execute_input":"2025-03-28T16:33:26.042124Z","iopub.status.idle":"2025-03-28T16:34:25.949016Z","shell.execute_reply.started":"2025-03-28T16:33:26.042104Z","shell.execute_reply":"2025-03-28T16:34:25.948309Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def seed_everything(seed):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.benchmark = True\n    torch.backends.cudnn.deterministic = True\nseed_everything(seed=5)\n\nstart_time = time.time()\ncutoff_time = start_time + (4 * 60 + 50) * 60\ncutoff_times = [int(x) for x in np.linspace(cutoff_time, start_time + 60 * 60, 50 + 1)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T16:34:56.976119Z","iopub.execute_input":"2025-03-28T16:34:56.97643Z","iopub.status.idle":"2025-03-28T16:34:56.98787Z","shell.execute_reply.started":"2025-03-28T16:34:56.976404Z","shell.execute_reply":"2025-03-28T16:34:56.987225Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if os.getenv('KAGGLE_KERNEL_RUN_TYPE') or os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    llm_model_pth = '/kaggle/input/m/shelterw/deepseek-r1/transformers/light-r1-7b-ds-awq/1'\nelse:\n    llm_model_pth = '/kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-7b-awq-casperhansen/1'\n\nMAX_NUM_SEQS = 75\nMAX_MODEL_LEN = 30000\n\nllm = LLM(\n    llm_model_pth,\n    #dtype=\"half\",                 # The data type for the model weights and activations\n    max_num_seqs=MAX_NUM_SEQS,    # Maximum number of sequences per iteration. Default is 256\n    max_model_len=MAX_MODEL_LEN,  # Model context length\n    trust_remote_code=True,       # Trust remote code (e.g., from HuggingFace) when downloading the model and tokenizer\n    tensor_parallel_size=4,       # The number of GPUs to use for distributed execution with tensor parallelism\n    gpu_memory_utilization=0.95,  # The ratio (between 0 and 1) of GPU memory to reserve for the model\n    seed=2025,\n)\n\ntokenizer = llm.get_tokenizer()","metadata":{"_kg_hide-output":true,"papermill":{"duration":237.271485,"end_time":"2024-12-14T00:03:19.252851","exception":false,"start_time":"2024-12-13T23:59:21.981366","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T16:36:46.976275Z","iopub.execute_input":"2025-03-28T16:36:46.976576Z","iopub.status.idle":"2025-03-28T16:38:46.13087Z","shell.execute_reply.started":"2025-03-28T16:36:46.976553Z","shell.execute_reply":"2025-03-28T16:38:46.130057Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_boxed_text(text):\n    pattern = r'\\\\boxed{([^}]*)}'\n    matches = re.findall(pattern, text)\n    if not matches:\n        return \"\"\n    for match in matches[::-1]:\n        if match != \"\":\n            return match\n    return \"\"\n\ndef batch_message_filter(list_of_messages) -> tuple[list[list[dict]], list[str]]:\n    extracted_answers = []\n    list_of_messages_to_keep = []\n    for messages in list_of_messages:\n        answer = extract_boxed_text(messages[-1]['content'])\n        if answer:\n            extracted_answers.append(answer)\n        else:\n            list_of_messages_to_keep.append(messages)\n    return list_of_messages_to_keep, extracted_answers\n\ndef select_answer(answers):\n    counter = Counter()\n    for answer in answers:\n        try:\n            if int(answer) == float(answer):\n                counter[int(answer)] += 1 + random.random() / 1_000\n        except:\n            pass\n    if not counter:\n        return 210\n    _, answer = sorted([(v,k) for k,v in counter.items()], reverse=True)[0]\n    return answer%1000\n\ndef batch_message_generate(list_of_messages) -> list[list[dict]]:\n    max_tokens = MAX_MODEL_LEN -2000\n    if time.time() > cutoff_times[-1]:\n        print(\"Speedrun\")\n        max_tokens = 2 * (MAX_MODEL_LEN - 2000 ) // 3\n\n    sampling_params = SamplingParams(\n        temperature=0.5,               # Randomness of the sampling\n        top_p=0.95,                    # Cumulative probability of the top tokens to consider\n        min_p=0.05,                    # Minimum probability for a token to be considered\n        #frequency_penalty=0.4,         # Balanced formula repetition control\n        #presence_penalty=0.3,\n        skip_special_tokens=True,      # Whether to skip special tokens in the output\n        max_tokens=max_tokens,         # Maximum number of tokens to generate\n        stop=[\"</think>\"],             # List of strings that stop the generation\n        seed=777,\n    )\n    \n    list_of_texts = [\n        tokenizer.apply_chat_template(\n            conversation=messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n        for messages in list_of_messages\n    ]\n\n    request_output = llm.generate(\n        prompts=list_of_texts,\n        sampling_params=sampling_params,\n    )\n    print([len(single_request_output.outputs[0].token_ids) for single_request_output in request_output])\n\n    sort_keys_and_list_of_messages = []\n    for messages, single_request_output in zip(list_of_messages, request_output):\n        #print()\n        #print(single_request_output.outputs[0].text)\n        #print()\n        messages.append({'role': 'assistant', 'content': single_request_output.outputs[0].text})\n\n        sort_keys_and_list_of_messages.append(\n            (\n                len(single_request_output.outputs[0].token_ids),\n                messages\n            )\n        )\n    print(f\"First sort : {[sort_key for sort_key, _ in sort_keys_and_list_of_messages]}\")\n    sort_keys_and_list_of_messages.sort(key=lambda sort_key_and_messages: sort_key_and_messages[0])\n    print(f\"Second sort: {[sort_key for sort_key, _ in sort_keys_and_list_of_messages]}\")\n    \n    list_of_messages = [messages for _, messages in sort_keys_and_list_of_messages]\n    return list_of_messages","metadata":{"papermill":{"duration":1.614384,"end_time":"2024-12-14T00:03:20.92038","exception":false,"start_time":"2024-12-14T00:03:19.305996","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T16:38:46.13202Z","iopub.execute_input":"2025-03-28T16:38:46.132279Z","iopub.status.idle":"2025-03-28T16:38:46.141839Z","shell.execute_reply.started":"2025-03-28T16:38:46.132254Z","shell.execute_reply":"2025-03-28T16:38:46.141247Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def batch_message_generate(list_of_messages) -> list[list[dict]]:\n    # Enforce single-sequence processing for iterative refinement\n    assert len(list_of_messages) == 1, \"Use sequential mode for iterative prompts\"\n    \n    # Dynamic token limits based on remaining time\n    max_tokens = MAX_MODEL_LEN\n    \n\n    # Configure sampling parameters\n    sampling_params = SamplingParams(\n        temperature=0.5,\n        top_p=0.95,\n        min_p=0.05,\n        skip_special_tokens=True,\n        max_tokens=max_tokens,\n        stop=[\"</think>\"],\n        seed=777,\n    )\n\n    # Convert message chain to prompt text\n    list_of_texts = [\n        tokenizer.apply_chat_template(\n            conversation=messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n        for messages in list_of_messages\n    ]\n\n    # Generate response using LLM\n    request_output = llm.generate(\n        prompts=list_of_texts,\n        sampling_params=sampling_params,\n    )\n\n    # Update message chain with generated response\n    updated_messages = []\n    for messages, single_request_output in zip(list_of_messages, request_output):\n        # Append assistant's response to message history\n        messages.append({\n            'role': 'assistant', \n            'content': single_request_output.outputs[0].text\n        })\n        \n        # Store token count for sorting (maintains compatibility)\n        updated_messages.append((\n            len(single_request_output.outputs[0].token_ids),\n            messages\n        ))\n\n    # Sort by generated response length (ascending)\n    updated_messages.sort(key=lambda x: x[0])\n    \n    # Return only the message chain (drop length metadata)\n    return [messages for _, messages in updated_messages]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompts = [#\"You must Take quality time to Verify every small cases upto the 7th value before assuming a pattern. There should be NO OVERSIGHT OR ASSUMPTION DUE TO GENERALIZATION. Summarize what you have done so far making sure every calculation is VERY ACCURATE!!! Complete the solution and arrive at the ONLY accurate answer after taking modulo 1000. Put your final answer within \\\\boxed{}.\",\n           #\"Solve this question step by step. Recheck every critical step for optimal accuracy. Do not mis‐simplify any expression. Summarize what you have done so far. Complete the solution. Take modulo 1000 of final answer. **IMPORTANT**: Put your final answer within \\\\boxed{}.\",\n           \"Break this question down first. Carefully Analyze and follow the problem statement exactly . Avoid: (- mis-calculations, - overly rough approximations, -imprecise handling of the radical–axis relations, - misapplication of an alternate counting method that over‐counted by introducing unnecessary factors, - incorrect generalization of the sum‐of‐digits formula over the whole range). **IMPORTANT**: Arrive at early answers. Before using an alternative method, Put possible answer within \\\\boxed{} after taking modulo 1000.\",\n           \"You MUST IGNORE  theoretical reasoning. You MUST TAKE QUALITY TIME TO PERFORM A real-time exhaustive verification of ALL CASEs using Sagemath or Sympy. **IMPORTANT**: Arrive at early answers. Before using an alternative method, Put possible answer within \\\\boxed{} after taking modulo 1000.\"\n          ]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T16:39:56.158451Z","iopub.execute_input":"2025-03-28T16:39:56.158796Z","iopub.status.idle":"2025-03-28T16:39:56.162252Z","shell.execute_reply.started":"2025-03-28T16:39:56.158771Z","shell.execute_reply":"2025-03-28T16:39:56.161581Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_for_question(question: str) -> int:\n    if time.time() > cutoff_time:\n        return 210\n\n    question += \" Please read the question again to understand it very well. Avoid off-by-one calculation\"\n    \n    messages = [{\"role\": \"system\", \"content\": \"\"}]\n    all_extracted_answers = []\n\n    for prompt_idx in range(2):\n        # Append current prompt configuration\n        messages.append({\n            \"role\": \"user\", \n            \"content\": f\"{prompts[prompt_idx]}\\n\\n{question}\"\n        })\n\n        # Generate and print response\n        messages = batch_message_generate([messages])[0]\n        \n        print(f\"\\n===== PROMPT {prompt_idx+1} RESPONSE =====\")\n        print(f\"Prompt Config: {prompts[prompt_idx][:100]}...\")  # Show first 100 chars of prompt\n        print(f\"Generated Response: {messages[-1]['content'][: -300]}...\")  # Show first 500 chars of response\n        print(\"-\"*70)\n\n        # Extract and store answer\n        _, extracted = batch_message_filter([messages])\n        all_extracted_answers.extend(extracted)\n        print(f\"Extracted answers: {all_extracted_answers}\")\n    \n    \n        \n        # Reset context while keeping system prompt\n        messages = [messages[0]] + [messages[-1]]  # Carry final response forward if needed\n\n    answer = select_answer(all_extracted_answers)\n    print(f\"Answer: {answer}\")\n    return answer % 1000","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict(id_: pl.DataFrame, question: pl.DataFrame) -> pl.DataFrame | pd.DataFrame:\n    id_ = id_.item(0)\n    print(\"------\")\n    print(id_)\n    question = question.item(0)\n    answer = predict_for_question(question)\n    print(question)\n    print(\"------\\n\\n\")\n    return pl.DataFrame({'id': id_, 'answer': answer})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.read_csv(\n    '/kaggle/input/ai-mathematical-olympiad-progress-prize-2/reference.csv'\n).drop('answer', axis=1).to_csv('reference.csv', index=False)","metadata":{"papermill":{"duration":0.070038,"end_time":"2024-12-14T00:03:21.108027","exception":false,"start_time":"2024-12-14T00:03:21.037989","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T16:40:02.43879Z","iopub.execute_input":"2025-03-28T16:40:02.439114Z","iopub.status.idle":"2025-03-28T16:40:02.476189Z","shell.execute_reply.started":"2025-03-28T16:40:02.439088Z","shell.execute_reply":"2025-03-28T16:40:02.475502Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inference_server = kaggle_evaluation.aimo_2_inference_server.AIMO2InferenceServer(predict)\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(\n        (\n#            '/kaggle/input/ai-mathematical-olympiad-progress-prize-2/test.csv',\n            'reference.csv',\n        )\n    )","metadata":{"papermill":{"duration":1644.24778,"end_time":"2024-12-14T00:30:45.363503","exception":false,"start_time":"2024-12-14T00:03:21.115723","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T16:40:02.919682Z","iopub.execute_input":"2025-03-28T16:40:02.920004Z","execution_failed":"2025-03-28T16:49:33.651Z"}},"outputs":[],"execution_count":null}]}
