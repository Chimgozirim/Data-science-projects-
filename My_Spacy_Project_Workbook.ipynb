{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8858a095-74da-48a1-9cb7-f07ef52d7439",
   "metadata": {},
   "source": [
    "# I will practice the use of the Spacy Library for NLP. This will also serve as my customized reference workbook for the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "03c1ed35-2aff-458c-9ddf-27dad25fa2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation\n",
    "\n",
    "#pip install spacy -> Already installed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc9f143d-0ed1-434a-946f-46ec25d40f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the library\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449cba83-01dd-4ec7-a6d1-75e3ef45d422",
   "metadata": {},
   "source": [
    "#### In the next cells, I will create a blank object 'nlp'(Because Spacy is an Object based library), and use apply word tokenization on a sample text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "542c7cc8-93ee-4333-aa39-cee071861af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a blank object in English language mainly for tokenization\n",
    "nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "3623c5d9-7a2f-45bc-8daa-05c90846b767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla\n",
      "'s\n",
      "gross\n",
      "cost\n",
      "of\n",
      "operating\n",
      "lease\n",
      "vehicles\n",
      "in\n",
      "FY2021\n",
      "Q1\n",
      "was\n",
      "$\n",
      "4.85\n",
      "billion\n",
      ".\n",
      "\n",
      "\n",
      "\"\n",
      "BMW\n",
      "'s\n",
      "\"\n",
      ",\n",
      "gross\n",
      "cost\n",
      "of\n",
      "operating\n",
      "vehicles\n",
      "in\n",
      "FY2021\n",
      "S1\n",
      "was\n",
      "$\n",
      "8\n",
      "billion\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "#This will be my text sample in a tuple for word tokenization\n",
    "text = (\n",
    "\"Tesla's\", '''gross cost of operating lease vehicles in FY2021 Q1 was $4.85 billion.\n",
    "\"BMW's\", gross cost of operating vehicles in FY2021 S1 was $8''', \"billion.\"\n",
    ")\n",
    "test = ' '.join(text) # -> convert to a string because the nlp object expects a string.\n",
    "#Creating a doc variable and printing word tokens using a Python 'for' loop.\n",
    "doca = nlp(test)\n",
    "for token in doca:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf073ea-e483-4def-9416-fecb535173e2",
   "metadata": {},
   "source": [
    "#### So basically the purpose of using a library like SpaCy for word tokenization is for unstructured text data. Because a list of words separated by commas will iterate separately according to their order in the the list with just a Python 'for' loop, because being in a list means they are already structured. So SpaCy takes a text sample (A group of words without a distinct separation except spaces) and separates (Tokenizes) each wprd accordingly. It will be a hectic work to try to organize words from a large text in a list, before iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "83d8fe36-3f1b-4dc4-827a-17de04d90fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Tesla\n",
      "'s\n",
      "gross\n",
      "cost\n",
      "of\n",
      "operating\n",
      "lease\n",
      "vehicles\n",
      "in\n",
      "FY2021\n",
      "Q1\n",
      "was\n",
      "$\n",
      "4.85\n",
      "billion\n",
      ".\n",
      "\n",
      "\n",
      "BMW\n",
      "'s\n",
      "gross\n",
      "cost\n",
      "of\n",
      "operating\n",
      "vehicles\n",
      "in\n",
      "FY2021\n",
      "S1\n",
      "was\n",
      "$\n",
      "8\n",
      "billion\n",
      ".\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#This will be my text sample for word tokenization\n",
    "text = '''\n",
    "Tesla's gross cost of operating lease vehicles in FY2021 Q1 was $4.85 billion.\n",
    "BMW's gross cost of operating vehicles in FY2021 S1 was $8 billion.\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a971ba3-3468-4e12-9d28-a24471c935ef",
   "metadata": {},
   "source": [
    "# Remember that the reason for word tokenization in NLP is for vector conversion, TF-IDF, embedding and encodings(I've not gotten most of these right, but it for the purpose of converting words to numbers for ML purposes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f4a97d-a4cf-43cc-9d07-7ec1ad572763",
   "metadata": {},
   "source": [
    "## Spacy has many attributes that can be used to extract words, digits, currency etc from a text.  \n",
    "#### In the next cells, I will use the Spacy object to extract urls and emails using text samples containing urls, and emails."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd8facc-47f6-47e2-bbe4-7b0afce86a8c",
   "metadata": {},
   "source": [
    "Extracting urls from a text sample using the nlp Spacy object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2b0d4ae0-2630-470f-8b90-261c810c1157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[https://twitter.com/elonmusk,\n",
       " https://www.tesla.com/.,\n",
       " https://twitter.com/teslarati,\n",
       " https://twitter.com/dummy_tesla,\n",
       " https://twitter.com/dummy_2_tesla]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Text sample:\n",
    "text1 = '''\n",
    "Follow our leader Elon musk on twitter here: https://twitter.com/elonmusk, more information \n",
    "on Tesla's products can be found at https://www.tesla.com/. Also here are leading influencers \n",
    "for tesla related news,\n",
    "https://twitter.com/teslarati\n",
    "https://twitter.com/dummy_tesla\n",
    "https://twitter.com/dummy_2_tesla\n",
    "'''\n",
    "\n",
    "doc = nlp(text1) # -> convert the text into a document\n",
    "print(type(doc)) #  -> This is to show that the text is now a doc. Should print spacy.tokens.doc.Doc\n",
    "\n",
    "url = [] # -> A global variable of empty list to contain the extracted url\n",
    "for token in doc:\n",
    "    if token.like_url: # -> This method checks for urls in the doc.\n",
    "        url.append(token)\n",
    "\n",
    "\n",
    "url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89826236-7711-4937-85c8-034aa3426942",
   "metadata": {},
   "source": [
    "Extracting emails from a text sample using the nlp Spacy object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "becb08bf-8acd-42b3-b765-305a6cb1e591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[john.doe@example.com,\n",
       " jane.smith@example.com,\n",
       " alice.johnson@example.com,\n",
       " robert.brown@example.com,\n",
       " emily.davis@example.com]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Text sample:\n",
    "text2 = '''\n",
    "1. **John Doe**  \n",
    "   - **Email:** john.doe@example.com  \n",
    "   - **Age:** 34  \n",
    "   - **Occupation:** Software Developer  \n",
    "   - **Location:** 123 Maple Street, Springfield  \n",
    "   - **Phone:** (555) 123-4567  \n",
    "\n",
    "2. **Jane Smith**  \n",
    "   - **Email:** jane.smith@example.com  \n",
    "   - **Age:** 29  \n",
    "   - **Occupation:** Marketing Specialist  \n",
    "   - **Location:** 456 Oak Avenue, Rivertown  \n",
    "   - **Phone:** (555) 987-6543  \n",
    "\n",
    "3. **Alice Johnson**  \n",
    "   - **Email:** alice.johnson@example.com  \n",
    "   - **Age:** 42  \n",
    "   - **Occupation:** Project Manager  \n",
    "   - **Location:** 789 Pine Road, Lakeview  \n",
    "   - **Phone:** (555) 234-5678  \n",
    "\n",
    "4. **Robert Brown**  \n",
    "   - **Email:** robert.brown@example.com  \n",
    "   - **Age:** 37  \n",
    "   - **Occupation:** Graphic Designer  \n",
    "   - **Location:** 321 Cedar Lane, Hilltop  \n",
    "   - **Phone:** (555) 345-6789  \n",
    "\n",
    "5. **Emily Davis**  \n",
    "   - **Email:** emily.davis@example.com  \n",
    "   - **Age:** 31  \n",
    "   - **Occupation:** Financial Analyst  \n",
    "   - **Location:** 654 Birch Boulevard, Greenwood  \n",
    "   - **Phone:** (555) 456-7890\n",
    "'''\n",
    "\n",
    "doc = nlp(text2) # -> convert the text into a document\n",
    "print(type(doc)) #  -> This is to show that the text is now a doc. It should print something like spacy.tokens.doc.Doc\n",
    "\n",
    "emails = [] # -> A global variable of empty list to contain the extracted url\n",
    "for token in doc:\n",
    "    if token.like_email: # -> This method checks for emails in the doc.\n",
    "        emails.append(token)\n",
    "\n",
    "\n",
    "emails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3075dd-7e6d-4f4f-9912-fbfbedf49db8",
   "metadata": {},
   "source": [
    "# Sentence tokenization or segmentation\n",
    "\n",
    "#### Spacy is also a sentence tokenizer. This means it takes a text/doc and smartly splits it into sentences. This is different from the usual splitting done with Python Pandas with where a delimeter is specified for the splitting. The difference is that with Pandas, abbreviations can be separated if the specified delimeter is a period(.), but Spacy is smart enough to know that such words are not sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2942850a-9d01-4cfa-93b4-4a1dd673b1d6",
   "metadata": {},
   "source": [
    "#### In the next cells, I'll use Spacy to apply a sentence tokenization on text samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "57a66357-0e18-47a7-879b-0bda58e2d13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text sample for \n",
    "text3 = '''\n",
    "Concentration of Risk: Credit Risk\n",
    "Financial instruments that potentially subject us to a concentration of credit risk consist of cash, cash equivalents, marketable securities,\n",
    "restricted cash, accounts receivable, convertible note hedges, and interest rate swaps. Our cash balances are primarily invested in money market funds\n",
    "or on deposit at high credit quality financial institutions in the U.S. These deposits are typically in excess of insured limits. As of September 30, 2021\n",
    "and December 31, 2020, no entity represented 10% or more of our total accounts receivable balance. The risk of concentration for our convertible note\n",
    "hedges and interest rate swaps is mitigated by transacting with several highly-rated multinational banks.\n",
    "Concentration of Risk: Supply Risk\n",
    "We are dependent on our suppliers, including single source suppliers, and the inability of these suppliers to deliver necessary components of our\n",
    "products in a timely manner at prices, quality levels and volumes acceptable to us, or our inability to efficiently manage these components from these\n",
    "suppliers, could have a material adverse effect on our business, prospects, financial condition and operating results.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1e9494-95a0-408b-b716-4d8ad2570c1c",
   "metadata": {},
   "source": [
    "#### The nlp object we created earlier in this notebook had nothing in its pipeline except work tokenization. To use it with the '.sents' method for sentence tokenizatin, we should have loaded it with 'en_core_web_sm' in this way: \n",
    "* #### nlp = spacy.load(\"en_core_web_sm\") .  This will load the  object with virtually everything needed in the pipline like the sentencizer, etc.\n",
    "\n",
    "#### On the other hand we can use the 'add_pipe' method to add a component like the 'sentencizer' (nlp.add_pipe(\"sentencizer\"), a 'sentence recognizer', or 'dependency parser' to the blank object, or set sentence boudaries, before using it for sentence tokenization. But the first approach is my prefered method, and the 'setencizer' has a way of making sentence tokenization that isn't really smart enough for me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "261f6b98-9a70-4e8f-932a-d6c956188671",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp.add_pipe(\"sentencizer\")\n",
    "#nlp.pipe_names # -> To check the contents of the pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d9eeaa8c-dc8a-4c22-8aa3-248856697989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Concentration of Risk: Credit Risk\n",
      "Financial instruments that potentially subject us to a concentration of credit risk consist of cash, cash equivalents, marketable securities,\n",
      "restricted cash, accounts receivable, convertible note hedges, and interest rate swaps.\n",
      "Our cash balances are primarily invested in money market funds\n",
      "or on deposit at high credit quality financial institutions in the U.S.\n",
      "These deposits are typically in excess of insured limits.\n",
      "As of September 30, 2021\n",
      "and December 31, 2020, no entity represented 10% or more of our total accounts receivable balance.\n",
      "The risk of concentration for our convertible note\n",
      "hedges and interest rate swaps is mitigated by transacting with several highly-rated multinational banks.\n",
      "\n",
      "Concentration of Risk: Supply Risk\n",
      "\n",
      "We are dependent on our suppliers, including single source suppliers, and the inability of these suppliers to deliver necessary components of our\n",
      "products in a timely manner at prices, quality levels and volumes acceptable to us, or our inability to efficiently manage these components from these\n",
      "suppliers, could have a material adverse effect on our business, prospects, financial condition and operating results.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\") # -> Recreating the 'nlp' object.\n",
    "\n",
    "doc = nlp(text3)\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d023089-8e5a-4169-99e8-70eeebc13c63",
   "metadata": {},
   "source": [
    "# Romoving spaces, punctuations and other characters using Spacy. \n",
    "\n",
    "### The difference between this method and using string.punctuation is that this method removes all the punctuations, and in some tasks it's not so proper, while using the string.punctuate method removes only the unicode punctuations while leaving the few ascii punctuations that might be needed for some certain tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea321c56-3e83-41ff-bf70-b964e7c23708",
   "metadata": {},
   "source": [
    "First, I will show how the parts of speech components works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "284d5b27-7bda-46b9-9e65-dd5a315f196c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'  |  PUNCT  |  punctuation  |  ``  |  opening quotation mark\n",
      "just  |  ADV  |  adverb  |  RB  |  adverb\n",
      "when  |  SCONJ  |  subordinating conjunction  |  WRB  |  wh-adverb\n",
      "i  |  PRON  |  pronoun  |  PRP  |  pronoun, personal\n",
      "think  |  VERB  |  verb  |  VBP  |  verb, non-3rd person singular present\n",
      "i  |  PRON  |  pronoun  |  PRP  |  pronoun, personal\n",
      "’ve  |  AUX  |  auxiliary  |  VBP  |  verb, non-3rd person singular present\n",
      "lost  |  VERB  |  verb  |  VBN  |  verb, past participle\n",
      "you  |  PRON  |  pronoun  |  PRP  |  pronoun, personal\n",
      "just  |  ADV  |  adverb  |  RB  |  adverb\n",
      "when  |  SCONJ  |  subordinating conjunction  |  WRB  |  wh-adverb\n",
      "i  |  PRON  |  pronoun  |  PRP  |  pronoun, personal\n",
      "’m  |  VERB  |  verb  |  VBZ  |  verb, 3rd person singular present\n",
      "so  |  ADV  |  adverb  |  RB  |  adverb\n",
      "tired  |  ADJ  |  adjective  |  JJ  |  adjective (English), other noun-modifier (Chinese)\n",
      "i  |  PRON  |  pronoun  |  PRP  |  pronoun, personal\n",
      "toss  |  VERB  |  verb  |  VBP  |  verb, non-3rd person singular present\n",
      "away  |  ADV  |  adverb  |  RB  |  adverb\n",
      "the  |  DET  |  determiner  |  DT  |  determiner\n",
      "fight  |  NOUN  |  noun  |  NN  |  noun, singular or mass\n",
      "and  |  CCONJ  |  coordinating conjunction  |  CC  |  conjunction, coordinating\n",
      "say  |  VERB  |  verb  |  VB  |  verb, base form\n",
      "“  |  PUNCT  |  punctuation  |  ``  |  opening quotation mark\n",
      "i  |  PRON  |  pronoun  |  PRP  |  pronoun, personal\n",
      "’ll  |  AUX  |  auxiliary  |  MD  |  verb, modal auxiliary\n",
      "just  |  ADV  |  adverb  |  RB  |  adverb\n",
      "embrace  |  VERB  |  verb  |  VB  |  verb, base form\n",
      "my  |  PRON  |  pronoun  |  PRP$  |  pronoun, possessive\n",
      "demons  |  NOUN  |  noun  |  NNS  |  noun, plural\n",
      "then  |  ADV  |  adverb  |  RB  |  adverb\n",
      "…  |  PUNCT  |  punctuation  |  ,  |  punctuation mark, comma\n",
      "‘  |  PUNCT  |  punctuation  |  ``  |  opening quotation mark\n",
      "cause  |  VERB  |  verb  |  VB  |  verb, base form\n",
      "you  |  PRON  |  pronoun  |  PRP  |  pronoun, personal\n",
      "feel  |  VERB  |  verb  |  VBP  |  verb, non-3rd person singular present\n",
      "so  |  ADV  |  adverb  |  RB  |  adverb\n",
      "far  |  ADV  |  adverb  |  RB  |  adverb\n",
      "away  |  ADV  |  adverb  |  RB  |  adverb\n",
      "and  |  CCONJ  |  coordinating conjunction  |  CC  |  conjunction, coordinating\n",
      "i  |  PRON  |  pronoun  |  PRP  |  pronoun, personal\n",
      "’ll  |  AUX  |  auxiliary  |  MD  |  verb, modal auxiliary\n",
      "never  |  ADV  |  adverb  |  RB  |  adverb\n",
      "be  |  AUX  |  auxiliary  |  VB  |  verb, base form\n",
      "your  |  PRON  |  pronoun  |  PRP$  |  pronoun, possessive\n",
      "angel  |  NOUN  |  noun  |  NN  |  noun, singular or mass\n",
      "”  |  PUNCT  |  punctuation  |  ''  |  closing quotation mark\n",
      "—  |  PUNCT  |  punctuation  |  :  |  punctuation mark, colon or ellipsis\n",
      "that  |  PRON  |  pronoun  |  DT  |  determiner\n",
      "’s  |  VERB  |  verb  |  VBZ  |  verb, 3rd person singular present\n",
      "when  |  SCONJ  |  subordinating conjunction  |  WRB  |  wh-adverb\n",
      "'  |  PUNCT  |  punctuation  |  ''  |  closing quotation mark\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\") # -> load the \"en_core_we_sm\" model in an object\n",
    "#Below, I have chosen to add the text directly as the parameter for the object instead of assigning it to a variable first.\n",
    "doc = nlp(\"'just when i think i’ve lost you just when i’m so tired i toss away the fight and say “i’ll just embrace my demons then… ‘cause you feel so far away and i’ll never be your angel” —that’s when' \")\n",
    "\n",
    "for token in doc:\n",
    "    print(token, \" | \", # -> The tokenized words\n",
    "          token.pos_, \" | \", # -> pos_ prints or reveals the parts of speech. \n",
    "          spacy.explain(token.pos_), \" | \", # -> spacy.explain() explains the meaning of the argument in its parenthesis. You use it to understand the spaCy methods.\n",
    "          token.tag_, \" | \", # -> .tag_ prints or reveals the type, tense and more detail of the parts of speech.\n",
    "          spacy.explain(token.tag_)\n",
    "         )  \n",
    "    #In spaCy, there are many of the parts of speech; More than the 8 fundamental ones. \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15785a9-f85c-439e-818b-bdddcf43eebe",
   "metadata": {},
   "source": [
    "#### Now to I will remove spaces, punctuations and other characters from the text, using Spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "b097964f-6471-44c9-8e5c-30d98f5dce2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[just,\n",
       " when,\n",
       " i,\n",
       " think,\n",
       " i,\n",
       " ’ve,\n",
       " lost,\n",
       " you,\n",
       " just,\n",
       " when,\n",
       " i,\n",
       " ’m,\n",
       " so,\n",
       " tired,\n",
       " i,\n",
       " toss,\n",
       " away,\n",
       " the,\n",
       " fight,\n",
       " and,\n",
       " say,\n",
       " i,\n",
       " ’ll,\n",
       " just,\n",
       " embrace,\n",
       " my,\n",
       " demons,\n",
       " then,\n",
       " cause,\n",
       " you,\n",
       " feel,\n",
       " so,\n",
       " far,\n",
       " away,\n",
       " and,\n",
       " i,\n",
       " ’ll,\n",
       " never,\n",
       " be,\n",
       " your,\n",
       " angel,\n",
       " that,\n",
       " ’s,\n",
       " when]"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\") # -> load the \"en_core_we_sm\" model in an object\n",
    "#Below, I have chosen to add the text directly as the parameter for the object instead of assigning it to a variable first.\n",
    "doc = nlp(\"'just when i think i’ve lost you just when i’m so tired i toss away the fight and say “i’ll just embrace my demons then… ‘cause you feel so far away and i’ll never be your angel” —that’s when' \")\n",
    "\n",
    "filtered_text = []\n",
    "for token in doc:\n",
    "    if token.pos_ not in [\"SPACE\", \"X\", \"PUNCT\"]: # -> pos_ prints or reveals the parts of speech. \n",
    "    #In spaCy, there are many of the. More than the 8 fundamental ones. \n",
    "    #In spaCy,\"SPACE\", \"X\", AND \"PUNCT\" represent spaces, special characters (like etc), and punctuations in a document.\n",
    "        filtered_text.append(token)     \n",
    "filtered_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d697a0-4825-430d-be36-67ee72c55c6e",
   "metadata": {},
   "source": [
    "#### Below is a Similar code to the one above, but here we want the filtered_text to be a clean list of strings rather than a spacy token, so we converted the token to text before appending it to the filtered_text list. Now we can extract the whole sentence from the list with the join method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "9d057c1e-a12e-4be3-8709-3e22cb3220a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'just when i think i ’ve lost you just when i ’m so tired i toss away the fight and say i ’ll just embrace my demons then cause you feel so far away and i ’ll never be your angel that ’s when'"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_text = []\n",
    "for token in doc:\n",
    "    if token.pos_ not in [\"SPACE\", \"X\", \"PUNCT\"]:\n",
    "        filtered_text.append(token.text)\n",
    "filtered_text = ' '.join(filtered_text)\n",
    "filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efbb5fe-6c6f-4601-9fb0-a80b10cd4e33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
